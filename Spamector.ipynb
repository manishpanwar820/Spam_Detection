{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let’s import the necessary libraries.\n",
    "from nltk.classify import NaiveBayesClassifier, SklearnClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "import nltk.classify.util\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.svm import LinearSVC\n",
    "import os, random, time, pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is our data directory\n",
    "data_directory = \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data ['enron1', 'enron2', 'enron3', 'enron4', 'enron5', 'enron6'] 0\n",
      "data\\enron1 ['ham', 'spam'] 0\n",
      "data\\enron1\\ham [] 3672\n",
      "data\\enron1\\spam [] 1500\n",
      "data\\enron2 ['ham', 'spam'] 0\n",
      "data\\enron2\\ham [] 4361\n",
      "data\\enron2\\spam [] 1496\n",
      "data\\enron3 ['ham', 'spam'] 0\n",
      "data\\enron3\\ham [] 4012\n",
      "data\\enron3\\spam [] 1500\n",
      "data\\enron4 ['ham', 'spam'] 0\n",
      "data\\enron4\\ham [] 1500\n",
      "data\\enron4\\spam [] 4500\n",
      "data\\enron5 ['ham', 'spam'] 0\n",
      "data\\enron5\\ham [] 1500\n",
      "data\\enron5\\spam [] 3675\n",
      "data\\enron6 ['ham', 'spam'] 0\n",
      "data\\enron6\\ham [] 1500\n",
      "data\\enron6\\spam [] 4500\n"
     ]
    }
   ],
   "source": [
    "# Let’s now loop through all the directories, subdirectories, and files in the above folder, and print them. \n",
    "# For files, we print the count.\n",
    "for directories, subdirs, files in os.walk(data_directory):\n",
    "    print(directories, subdirs, len(files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2**\n",
    "\n",
    "* Printing the files when we are in the ham or spam folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\enron1\\ham [] 3672\n",
      "data\\enron1\\spam [] 1500\n",
      "data\\enron2\\ham [] 4361\n",
      "data\\enron2\\spam [] 1496\n",
      "data\\enron3\\ham [] 4012\n",
      "data\\enron3\\spam [] 1500\n",
      "data\\enron4\\ham [] 1500\n",
      "data\\enron4\\spam [] 4500\n",
      "data\\enron5\\ham [] 1500\n",
      "data\\enron5\\spam [] 3675\n",
      "data\\enron6\\ham [] 1500\n",
      "data\\enron6\\spam [] 4500\n"
     ]
    }
   ],
   "source": [
    "# Now instead of printing all files and folders, we only print the files when we are in the ham or spam folder.\n",
    "for directories, subdirs, files in os.walk(data_directory):\n",
    "    if (os.path.split(directories)[1]  == 'spam'):\n",
    "        print(directories, subdirs, len(files))\n",
    "\n",
    "    if (os.path.split(directories)[1]  == 'ham'):\n",
    "        print(directories, subdirs, len(files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why does it matter?**\n",
    "\n",
    "* When we start reading the files, we want to make sure we are only reading them from the spam and ham folders.\n",
    "\n",
    "**Step 3**\n",
    "\n",
    "* Reading all the files in those folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: christmas tree farm pictures\n",
      "\n",
      "--------- \n",
      "Subject: dobmeos with hgh my energy level has gone up ! stukm\n",
      "introducing\n",
      "doctor - formulated\n",
      "hgh\n",
      "human growth hormone - also called hgh\n",
      "is referred to in medical science as the master hormone . it is very plentiful\n",
      "when we are young , but near the age of twenty - one our bodies begin to produce\n",
      "less of it . by the time we are forty nearly everyone is deficient in hgh ,\n",
      "and at eighty our production has normally diminished at least 90 - 95 % .\n",
      "advantages of hgh :\n",
      "- increased muscle strength\n",
      "- loss in body fat\n",
      "- increased bone density\n",
      "- lower blood pressure\n",
      "- quickens wound healing\n",
      "- reduces cellulite\n",
      "- improved vision\n",
      "- wrinkle disappearance\n",
      "- increased skin thickness texture\n",
      "- increased energy levels\n",
      "- improved sleep and emotional stability\n",
      "- improved memory and mental alertness\n",
      "- increased sexual potency\n",
      "- resistance to common illness\n",
      "- strengthened heart muscle\n",
      "- controlled cholesterol\n",
      "- controlled mood swings\n",
      "- new hair growth and color restore\n",
      "read\n",
      "more at this website\n",
      "unsubscribe\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We read the files and append them to the ham and spam list\n",
    "hamList = []\n",
    "spamList = []\n",
    "for directories, subdirs, files in os.walk(data_directory):\n",
    "    if (os.path.split(directories)[1]  == 'ham'):\n",
    "        for fileName in files:      \n",
    "            with open(os.path.join(directories, fileName), encoding=\"latin-1\") as f:\n",
    "                message = f.read()\n",
    "                hamList.append(message)\n",
    "\n",
    "    if (os.path.split(directories)[1]  == 'spam'):\n",
    "        for fileName in files:\n",
    "            with open(os.path.join(directories, fileName), encoding=\"latin-1\") as f:\n",
    "                message = f.read()\n",
    "                spamList.append(message)\n",
    "print(hamList[0])\n",
    "print('--------- ')\n",
    "print(spamList[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createWordFeatures(words):\n",
    "    my_dict = dict( [ (word, True) for word in words] )\n",
    "    return my_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's test our function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MIS 5470': True,\n",
       " 'R': True,\n",
       " 'and': True,\n",
       " 'class': True,\n",
       " 'has': True,\n",
       " 'modules': True,\n",
       " 'python': True,\n",
       " 'the': True}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "createWordFeatures([\"the\", \"MIS 5470\", \"class\", \"has\", \"python\", \"and\",\"R\",\"modules\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2**\n",
    "\n",
    "* Now with the help of *createWordFeatures()* function, we will append a “ham” at the end. This is to tell the algorithm that this text is of type ham, and we’ll do the same for “spam”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'Subject': True, ':': True, 'christmas': True, 'tree': True, 'farm': True, 'pictures': True}, 'ham')\n",
      "({'Subject': True, ':': True, 'dobmeos': True, 'with': True, 'hgh': True, 'my': True, 'energy': True, 'level': True, 'has': True, 'gone': True, 'up': True, '!': True, 'stukm': True, 'introducing': True, 'doctor': True, '-': True, 'formulated': True, 'human': True, 'growth': True, 'hormone': True, 'also': True, 'called': True, 'is': True, 'referred': True, 'to': True, 'in': True, 'medical': True, 'science': True, 'as': True, 'the': True, 'master': True, '.': True, 'it': True, 'very': True, 'plentiful': True, 'when': True, 'we': True, 'are': True, 'young': True, ',': True, 'but': True, 'near': True, 'age': True, 'of': True, 'twenty': True, 'one': True, 'our': True, 'bodies': True, 'begin': True, 'produce': True, 'less': True, 'by': True, 'time': True, 'forty': True, 'nearly': True, 'everyone': True, 'deficient': True, 'and': True, 'at': True, 'eighty': True, 'production': True, 'normally': True, 'diminished': True, 'least': True, '90': True, '95': True, '%': True, 'advantages': True, 'increased': True, 'muscle': True, 'strength': True, 'loss': True, 'body': True, 'fat': True, 'bone': True, 'density': True, 'lower': True, 'blood': True, 'pressure': True, 'quickens': True, 'wound': True, 'healing': True, 'reduces': True, 'cellulite': True, 'improved': True, 'vision': True, 'wrinkle': True, 'disappearance': True, 'skin': True, 'thickness': True, 'texture': True, 'levels': True, 'sleep': True, 'emotional': True, 'stability': True, 'memory': True, 'mental': True, 'alertness': True, 'sexual': True, 'potency': True, 'resistance': True, 'common': True, 'illness': True, 'strengthened': True, 'heart': True, 'controlled': True, 'cholesterol': True, 'mood': True, 'swings': True, 'new': True, 'hair': True, 'color': True, 'restore': True, 'read': True, 'more': True, 'this': True, 'website': True, 'unsubscribe': True}, 'spam')\n"
     ]
    }
   ],
   "source": [
    "# Fist, using word_tokenize(), we break the sentences into words.\n",
    "# Second, we use createWordFeatures() function.\n",
    "hamList = []\n",
    "spamList = []\n",
    "for directories, subdirs, files in os.walk(data_directory):\n",
    "    if (os.path.split(directories)[1]  == 'ham'):\n",
    "        for fileName in files:      \n",
    "            with open(os.path.join(directories, fileName), encoding=\"latin-1\") as f:\n",
    "                message = f.read()               \n",
    "                wordList = word_tokenize(message)               \n",
    "                hamList.append((createWordFeatures(wordList), \"ham\"))\n",
    "    \n",
    "    if (os.path.split(directories)[1]  == 'spam'):\n",
    "        for fileName in files:\n",
    "            with open(os.path.join(directories, fileName), encoding=\"latin-1\") as f:\n",
    "                message = f.read()               \n",
    "                wordList = word_tokenize(message)               \n",
    "                spamList.append((createWordFeatures(wordList), \"spam\"))\n",
    "print(hamList[0])\n",
    "print(spamList[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3**\n",
    "\n",
    "* Merging spam and ham lists.\n",
    "* Shuffling the result to make it randomised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files: 33716\n"
     ]
    }
   ],
   "source": [
    "mergedList = hamList + spamList\n",
    "random.shuffle(mergedList)\n",
    "print(\"Number of files: {}\".format(len(mergedList)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling Data \n",
    "***\n",
    "**Step 1**\n",
    "\n",
    "* Creating test and train splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of files:    33716\n",
      "Number of Training files: 20229\n",
      "Number of Test files:     13487\n"
     ]
    }
   ],
   "source": [
    "# We choose to have 60% of the data as training and 40% as test.\n",
    "trainingPart = int(len(mergedList) * .6)\n",
    "trainingData = mergedList[:trainingPart]\n",
    "testData =  mergedList[trainingPart:]\n",
    "\n",
    "print(\"Total Number of files:    {}\".format(len(mergedList)))\n",
    "print(\"Number of Training files: {}\".format(len(trainingData)))\n",
    "print(\"Number of Test files:     {}\".format(len(testData)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2**\n",
    "\n",
    "* Calling the Naive Bayes classifier.\n",
    "* Finding its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained the Naive Bayes classifier in 7.60 seconds\n",
      "Accuracy is:  98.61347964706755\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "nb_classifier = NaiveBayesClassifier.train(trainingData)\n",
    "print(\"Trained the Naive Bayes classifier in {:,.2f} seconds\".format(time.time()-start_time))\n",
    "accuracy = nltk.classify.util.accuracy(nb_classifier, testData)\n",
    "print(\"Accuracy is: \", accuracy * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We are getting an accuracy of 98%, which is Great.\n",
    "\n",
    "**Step 3**\n",
    "\n",
    "* Let's have a look at the most interesting features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                   enron = True              ham : spam   =   3115.4 : 1.0\n",
      "                   daren = True              ham : spam   =    447.4 : 1.0\n",
      "                     php = True             spam : ham    =    363.0 : 1.0\n",
      "                     hpl = True              ham : spam   =    305.3 : 1.0\n",
      "                     ect = True              ham : spam   =    219.8 : 1.0\n",
      "                crenshaw = True              ham : spam   =    217.0 : 1.0\n",
      "                   corel = True             spam : ham    =    200.7 : 1.0\n",
      "                     713 = True              ham : spam   =    188.3 : 1.0\n",
      "              scheduling = True              ham : spam   =    181.1 : 1.0\n",
      "                     eol = True              ham : spam   =    179.0 : 1.0\n",
      "                  louise = True              ham : spam   =    177.8 : 1.0\n",
      "                 parsing = True              ham : spam   =    172.0 : 1.0\n",
      "                     sex = True             spam : ham    =    158.7 : 1.0\n",
      "               schedules = True              ham : spam   =    158.3 : 1.0\n",
      "                       \u0001 = True              ham : spam   =    154.2 : 1.0\n",
      "                      xp = True             spam : ham    =    146.7 : 1.0\n",
      "             origination = True              ham : spam   =    130.1 : 1.0\n",
      "              medication = True             spam : ham    =    122.7 : 1.0\n",
      "            prescription = True             spam : ham    =    104.7 : 1.0\n",
      "                    omit = True             spam : ham    =    104.0 : 1.0\n"
     ]
    }
   ],
   "source": [
    "nb_classifier.show_most_informative_features(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4**\n",
    "\n",
    "* Calling the Linear SVC.\n",
    "* Finding its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained the SVM classifier in 100.69 seconds\n",
      "Accuracy is:  98.32431230073404\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "SVM_classifier = SklearnClassifier(LinearSVC(), sparse=False).train(trainingData)\n",
    "print(\"Trained the SVM classifier in {:,.2f} seconds\".format(time.time()-start_time))\n",
    "SVM_accuracy = nltk.classify.util.accuracy(SVM_classifier, testData)\n",
    "print(\"Accuracy is: \", SVM_accuracy * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting New Emails\n",
    "\n",
    "* Let's clasify the below messages as spam or ham. How to do it?\n",
    "\n",
    "\n",
    "1. Break the message into words using *word_tokenzise*\n",
    "2. Generate features using *createWordFeatures*\n",
    "3. Use the *classify* function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of Nigerian Scam email\n",
    "msg1 = '''Sir, we are honourably seeking your assistance in the following ways.\n",
    "        1) To provide a Bank account where this money would be transferred to.\n",
    "        2) To serve as the guardian of this since I am a girl of 26 years.\n",
    "        Moreover Sir, we are willing to offer you 15% of the sum as compensation for effort input after the successful transfer of this fund to your designate account overseas. please feel free to contact ,me via this email address\n",
    "        wumi1000abdul@yahoo.comAnticipating to hear from you soon.Thanks and God Bless.'''\n",
    "\n",
    "# Note from Professor regarding Python + Tableau for plotting flight pattern\n",
    "msg2 = '''I've mentioned that Python is widely used for data pre-processing tasks. \n",
    "        Here's a really nice use of Python for preparing flight data for plotting in Tableau. \n",
    "        The same principles can be used for plotting all kinds of geographic related \"routes\". \n",
    "        Plus, you learn about KML files and working with spatial data.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message 1 is spam\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(msg1)\n",
    "features = createWordFeatures(words)\n",
    "print(\"Message 1 is\" ,nb_classifier.classify(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message 2 is ham\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(msg2)\n",
    "features = createWordFeatures(words)\n",
    "print(\"Message 2 is\" ,nb_classifier.classify(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model to disk\n",
    "filename = 'nb_model.sav'\n",
    "pickle.dump(nb_classifier, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the model with Pickle in 2.54 seconds\n"
     ]
    }
   ],
   "source": [
    "# Load the model from disk\n",
    "start_time_pickle = time.time()\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "print(\"Loaded the model with Pickle in {:,.2f} seconds\".format(time.time()-start_time_pickle))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can use as well **Sklearn’s Joblib**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nb_model.joblib']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Saving the model with Joblib\n",
    "joblib.dump(nb_classifier,\"nb_model.joblib\", compress=1) # compression into 1 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the model with Joblib in 13.73 seconds\n"
     ]
    }
   ],
   "source": [
    "# Loading the model with Joblib\n",
    "start_time_joblib = time.time()\n",
    "joblib_model = joblib.load(\"nb_model.joblib\")\n",
    "print(\"Loaded the model with Joblib in {:,.2f} seconds\".format(time.time()-start_time_joblib))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Application\n",
    "***\n",
    "<img src=\"jupyter_images/web.gif\"  style=\"width: 250px;\"/>\n",
    "*  Let's now create a simple web application that would allow us to enter any message as an input and tell us if it's a spam or ham. We'll be using Flask, a Python web application micro-framework.\n",
    "\n",
    "\n",
    "**“Hello World” application in Flask**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route(\"/\")\n",
    "def hello():\n",
    "    return \"Hello 5470 World!\"\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "#    app.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure\n",
    "\n",
    "This is the basic structure of our web application:\n",
    "![Structure](jupyter_images/structure.png)\n",
    "\n",
    "* The **templates** folder is the place where the templates will be put. The **static** folder is the place where any files (images, css, javascript) needed by the web application will be put.\n",
    "\n",
    "* **app.py**: It's our main Python Application.\n",
    "\n",
    "* **engine.py**: It's the Python class containing the predictive model and its related functions.\n",
    "***\n",
    "## Engine ~ Spam Detector as a Class\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpamDetector:\n",
    "\n",
    "\tdef createWordFeatures(self,words):\n",
    "\t\t\"\"\"\n",
    "\t\tFunction\n",
    "\t\t--------\n",
    "\t\tcreateWordFeatures\n",
    "\n",
    "\t\tCreate a dictionary that returns True for each word.\n",
    "\n",
    "\t\tParameters\n",
    "\t\t----------\n",
    "\t\twords : list\n",
    "\t\tThe list of words\n",
    "\n",
    "\t\tReturns\n",
    "\t\t-------\n",
    "\t\tresDict : Dict\n",
    "\n",
    "\t\tExample\n",
    "\t\t-------\n",
    "\t\tcreateWordFeatures([\"the\", \"MIS 5470\", \"class\", \"has\", \"python\", \"and\",\"R\",\"modules\"])\n",
    "\t\t\n",
    "\t\t{'MIS 5470': True,\n",
    "\t\t 'R': True,\n",
    "\t\t 'and': True,\n",
    "\t\t 'class': True,\n",
    "\t\t 'has': True,\n",
    "\t\t 'modules': True,\n",
    "\t\t 'python': True,\n",
    "\t\t 'the': True}\n",
    "\t\t\"\"\"\n",
    "\t\tresDict = dict( [ (word, True) for word in words] )\n",
    "\t\treturn resDict\n",
    "\t\t\n",
    "\tdef tokenizeCreateWordFeatures(self):\n",
    "\t\t\"\"\"\n",
    "\t\tFunction\n",
    "\t\t--------\n",
    "\t\ttokenizeCreateWordFeatures\n",
    "\n",
    "\t\tAppend a \"ham\" or \"spam\" at the end of each dictionary created by createWordFeatures function. \n",
    "\t\tThis is to tell the algorithm that this text is of type ham or spam.\n",
    "\t\tMerge spam and ham lists.\n",
    "\t\tShuffle the result to make it randomised.\n",
    "\n",
    "\t\tReturns\n",
    "\t\t-------\n",
    "\t\tmergedList : list\n",
    "\n",
    "\t\tExample\n",
    "\t\t-------\n",
    "\t\toutput = tokenizeCreateWordFeatures()\n",
    "\t\tprint(output[0])\n",
    "\t\t\n",
    "\t\t({'Subject': True, ':': True, 'fw': True, 'nice': True, 'mhoter': True, 'fucking': True, \n",
    "\t\t'top': True, 'of': True, 'the': True, 'morning': True, 'to': True, 'you': True, '!': True,\n",
    "\t\t')': True, 'matayaa': True}, 'spam')\n",
    "\t\t\"\"\"\n",
    "\t\tdata_directory = \"data\"\t\t\n",
    "\t\thamList = []\n",
    "\t\tspamList = []\n",
    "\t\tfor directories, subdirs, files in os.walk(data_directory):\n",
    "\t\t\tif (os.path.split(directories)[1]  == 'ham'):\n",
    "\t\t\t\tfor fileName in files:      \n",
    "\t\t\t\t\twith open(os.path.join(directories, fileName), encoding=\"latin-1\") as f:\n",
    "\t\t\t\t\t\tmessage = f.read()\t\t\t\t\t\n",
    "\t\t\t\t\t\twords = word_tokenize(message)\t\t\t\t\t\t\n",
    "\t\t\t\t\t\thamList.append((self.createWordFeatures(words), \"ham\"))\n",
    "\t\t\t\n",
    "\t\t\tif (os.path.split(directories)[1]  == 'spam'):\n",
    "\t\t\t\tfor fileName in files:\n",
    "\t\t\t\t\twith open(os.path.join(directories, fileName), encoding=\"latin-1\") as f:\n",
    "\t\t\t\t\t\tmessage = f.read()\t\t\t\t\t\t\n",
    "\t\t\t\t\t\twords = word_tokenize(message)\t\t\t\t\t\t\n",
    "\t\t\t\t\t\tspamList.append((self.createWordFeatures(words), \"spam\"))\n",
    "\t\tmergedList = hamList + spamList\n",
    "\t\trandom.shuffle(mergedList)\t\t\n",
    "\t\treturn mergedList\n",
    "\n",
    "\tdef createTestTrain(self):\n",
    "\t\t\"\"\"\n",
    "\t\tFunction\n",
    "\t\t--------\n",
    "\t\tcreateTestTrain\n",
    "\n",
    "\t\tCreate test/train splits.\n",
    "\n",
    "\t\tReturns\n",
    "\t\t-------\n",
    "\t\t(trainingData, testData) : tuple\n",
    "\t\t\"\"\"\n",
    "\t\tmergedList = self.tokenizeCreateWordFeatures()\n",
    "\t\ttrainingPart = int(len(mergedList) * .6)\n",
    "\t\ttrainingData = mergedList[:trainingPart]\n",
    "\t\ttestData =  mergedList[trainingPart:]\t\t\n",
    "\t\treturn (trainingData, testData)\n",
    "\n",
    "\n",
    "\tdef createModel(self):\n",
    "\t\t\"\"\"\n",
    "\t\tFunction\n",
    "\t\t--------\n",
    "\t\tcreateModel\n",
    "\n",
    "\t\tCreate the naive Bayes classifier\n",
    "\n",
    "\t\tReturns\n",
    "\t\t-------\n",
    "\t\tclassifier : nltk.classify.naivebayes.NaiveBayesClassifier\n",
    "\t\t\"\"\"\n",
    "\t\ttrainingData, testData = self.createTestTrain()\t\t\n",
    "\t\tclassifier = NaiveBayesClassifier.train(trainingData)\n",
    "\t\treturn classifier\n",
    "\n",
    "\tdef saveModel(self):\n",
    "\t\t\"\"\"\n",
    "\t\tFunction\n",
    "\t\t--------\n",
    "\t\tsaveModel\n",
    "\n",
    "\t\tSave the model to disk\n",
    "\n",
    "\t\tReturns\n",
    "\t\t-------\n",
    "\t\toutputMsg : str\n",
    "\t\t\n",
    "\t\tExample\n",
    "\t\t-------\n",
    "\t\tsaveModel()\n",
    "\t\t'Model saved successfully !'\n",
    "\t\t\"\"\"\n",
    "\t\tclassifier = self.createModel()\n",
    "\t\tfileName = 'nb_model.sav'\n",
    "\t\toutputMsg = \"\"\n",
    "\t\ttry:\n",
    "\t\t\tpickle.dump(classifier, open(fileName, 'wb'))\n",
    "\t\t\toutputMsg = \"Model saved successfully !\"\n",
    "\t\texcept:\n",
    "\t\t\toutputMsg = \"Error when saving model !\"\n",
    "\t\treturn outputMsg\n",
    "\n",
    "\tdef predictMessage(self,msg):\n",
    "\t\t\"\"\"\n",
    "\t\tFunction\n",
    "\t\t--------\n",
    "\t\tpredictMessage\n",
    "\n",
    "\t\tClassify the message by telling if it is a ham or spam.\n",
    "\n",
    "\t\tParameters\n",
    "\t\t----------\n",
    "\t\tmsg : str\n",
    "\t\tA message we want to classify\n",
    "\t\t\n",
    "\t\tReturns\n",
    "\t\t-------\n",
    "\t\tstr: 'ham' or 'spam'\n",
    "\t\t\n",
    "\t\tExample\n",
    "\t\t-------\n",
    "\t\tmessage = \"Hi welcome to this session, please log in using your username and password then click on the start button.\"\n",
    "\t\tpredictMessage(message)\n",
    "\t\t\n",
    "\t\t'ham'\n",
    "\t\t\"\"\"\n",
    "\t\tfileName = 'nb_model.sav'\n",
    "\t\tloaded_model = pickle.load(open(fileName, 'rb'))\n",
    "\t\toutput_msg = \"\"\n",
    "\t\twords = word_tokenize(msg)\n",
    "\t\toutput_msg = dict( [ (word, True) for word in words] )\n",
    "\t\treturn loaded_model.classify(output_msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## App ~ Flask Server\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, render_template, json, redirect, url_for, session, abort, Markup\n",
    "from engine import SpamDetector\n",
    "app = Flask(__name__)\n",
    "app.secret_key = 'F12Zr47j\\3yX R~X@H!jmM]Lwf/,?KT'\n",
    "\n",
    "# Defining the basic route and its corresponding request handler\n",
    "@app.route(\"/\")\n",
    "def index():\n",
    "\n",
    "    return render_template('index.html')\n",
    "\n",
    "\n",
    "# Run the engine\n",
    "@app.route(\"/RunEngine\", methods=['POST', 'GET'])\n",
    "def RunEngine():\n",
    "\n",
    "    clearsession()\n",
    "    OutputMsg = None\n",
    "    message = None\n",
    "\n",
    "    if request.form['inputMessage'] !=\"\":\n",
    "\n",
    "        message = request.form['inputMessage']\n",
    "        spam_detector = SpamDetector()\n",
    "        OutputMsg = spam_detector.predictMessage(message)\n",
    "        if OutputMsg == \"ham\":\n",
    "            OutputMsg = Markup(\"<img src='../static/img/ham.png' width='50' height='50'/><br>No worries, your message is fine ðŸ˜‰\")\n",
    "        else:\n",
    "            OutputMsg = Markup(\"<img src='../static/img/spam.png' width='50' height='50'/><br>Be careful! your message is a spam!\")\n",
    "        return render_template('index.html', message=message, OutputMsg=OutputMsg)\n",
    "\n",
    "    else:\n",
    "        return redirect(url_for('index'))\n",
    "\n",
    "# Clear the session \n",
    "@app.route('/clear')\n",
    "def clearsession():\n",
    "    # Clear the session\n",
    "    session.clear()\n",
    "    # Redirect the user to the main page\n",
    "    return redirect(url_for('index'))\n",
    "\n",
    "# Build the model \n",
    "@app.route('/build')\n",
    "def build_model():\n",
    "    spam_detector = SpamDetector()\n",
    "    build_results = spam_detector.saveModel()\n",
    "    return build_results\n",
    "\n",
    "# Predict the message via command line\n",
    "@app.route('/predict/<message>')\n",
    "def predict_message(message):\n",
    "    spam_detector = SpamDetector()\n",
    "    return spam_detector.predictMessage(message)\n",
    "\n",
    "# Checking if the executed file is the main program and run the app\t\n",
    "#if __name__ == \"__main__\":\n",
    "    #app.debug=True\n",
    "    #app.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Page\n",
    "***\n",
    "<img src=\"jupyter_images/webpage.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Run It\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python app.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spamector as API\n",
    "***\n",
    "\n",
    "We have the possibility to use Spamector as an API. Here is an example using [Postman](https://www.getpostman.com/) and we are trying to classify the message passed in the URL.\n",
    "<img src=\"jupyter_images/postman.png\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
